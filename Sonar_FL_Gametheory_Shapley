pip install shap pandas scikit-learn


import pandas as pd
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Load dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
dataset = pd.read_csv(url, header=None)

# Split data into X and y
X = dataset.iloc[:, 0:60].values
y = dataset.iloc[:, 60].values

# Encode the target labels
encoder = LabelEncoder()
y = encoder.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a RandomForest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=0)
clf.fit(X_train, y_train)
# Create a Tree explainer and compute SHAP values
explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(X_test)

# Visualize the SHAP values for the first instance in the test set
shap.initjs()
shap.force_plot(explainer.expected_value[0], shap_values[0][0], X_test[0])

# Summary plot for feature importances
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Get the mean absolute shap values for each feature
mean_shap_values = np.mean(np.abs(shap_values[0]), axis=0)

# Sort the features by shap value in descending order
feature_importance = pd.DataFrame(list(zip(range(X.shape[1]), mean_shap_values)), columns=['feature', 'importance'])
feature_importance = feature_importance.sort_values('importance', ascending=False)

# Print the top 10 features by shap value
print(feature_importance.head(10))

# Select the top 10 features
top_features = feature_importance['feature'].head(20).tolist()

# Create a new dataset with only the top features
X_top = X[:, top_features]

# Split the dataset into training, validation, and testing sets
X_temp, X_test, y_temp, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2




import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
from keras.constraints import maxnorm
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
from sklearn.metrics import roc_auc_score, roc_curve
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint



import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
from keras.constraints import maxnorm
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
from sklearn.metrics import roc_auc_score, roc_curve
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint



import numpy as np
import tensorflow as tf
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score

# Initialize structures for metrics storage
serverhist1 = {
    "loss": [],
    "accuracy": [],
    "payoff": [],
    "f1_score": [],
    "precision": [],
    "recall": [],
    "auc": []
}
client_payoff = []
client_number = []
all_rounds_payoffs = []
confusion_matrices = []
global_accuracies = []
global_accuracy_final = []
global_losses = []
auc_list = []
def GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr):
    input_shape = X_train.shape[1:]
    model_list = []

    for i in range(num_clients):
        model = tf.keras.models.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=input_shape),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])
        model_list.append(model)

    # Added lists to store global metrics
    global_losses = []
    global_accuracies = []

    for epoch in range(num_epochs):
        for i in range(num_clients):
            model_list[i].fit(
                X_train[i * (len(X_train) // num_clients):(i + 1) * (len(X_train) // num_clients)],
                y_train[i * (len(X_train) // num_clients):(i + 1) * (len(X_train) // num_clients)],
                epochs=client_epochs,
                batch_size=batch_size,
                validation_data=(X_val, y_val)
            )

            y_pred = model_list[i].predict(X_test)
            y_pred_binary = (y_pred > 0.5).astype(int)
            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_binary, average='binary')
            cm = confusion_matrix(y_test, y_pred_binary)
            auc = roc_auc_score(y_test, y_pred)

            h = model_list[i].evaluate(X_test, y_test)

            serverhist1['loss'].append(h[0])
            serverhist1['accuracy'].append(h[1])
            serverhist1['precision'].append(precision)
            serverhist1['recall'].append(recall)
            serverhist1['f1_score'].append(f1)
            serverhist1['auc'].append(auc)

        client_accuracies = []
        for i in range(num_clients):
            _, accuracy = model_list[i].evaluate(X_test, y_test, verbose=0)
            client_accuracies.append(accuracy)

        client_payoffs = []
        for i in range(num_clients):
            payoff = client_accuracies[i] - np.mean(client_accuracies)
            client_payoffs.append(payoff)
            client_payoff.append(payoff)
            client_number.append(i)
            serverhist1['payoff'].append(client_payoffs[i])

        all_rounds_payoffs.append(client_payoffs)
        sorted_clients = sorted(range(num_clients), key=lambda x: client_payoffs[x], reverse=True)
        print("sorted clients",sorted_clients)
        selected_clients = sorted_clients[:num_clients_to_select]
        selected_payoffs = [client_payoffs[i] for i in selected_clients]
        weights = [client_payoffs[i] for i in selected_clients]
        print("Payoffs of selected clients:", selected_payoffs)
        weights /= np.sum(weights)

        #sorted_clients = sorted(range(num_clients), key=lambda x: client_payoffs[x], reverse=True)
        #print("sorted clients",sorted_clients)
        #selected_clients = sorted_clients[:num_clients_to_select]
        #print("selected clients len", selected_clients)
        #selected_payoffs = [client_payoffs[i] for i in selected_clients]
        #weights = [client_payoffs[i] for i in selected_clients]
        #print("Payoffs of selected clients:", selected_payoffs)
        #weights /= np.sum(weights)

        #global_weights = np.zeros_like(model_list[0].get_weights())
        #for i in range(num_clients_to_select):
         #   global_weights += weights[i] * np.array(model_list[i].get_weights())
        #new_global_weights = global_weights / num_clients_to_select
        # Aggregate client model weights based on their performance/payoff
        global_weights = np.zeros_like(model_list[0].get_weights())
        for i in range(num_clients_to_select):
            global_weights += weights[i] * np.array(model_list[i].get_weights())
        new_global_weights = global_weights / num_clients_to_select

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)
          #  for layer in model_list[i].layers:
           #     layer.set_weights(layer.get_weights() + lr * client_payoffs[i])

        #global_weights = np.array(model_list[0].get_weights())
        for i in range(1, num_clients):
            global_weights += np.array(model_list[i].get_weights())
        new_global_weights = global_weights / num_clients

        for i in range(num_clients):
            model_list[i].set_weights(new_global_weights)

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)
          #  for layer in model_list[i].layers:
           #     layer.set_weights(layer.get_weights() + lr * client_payoffs[i])

        #global_weights = np.array(model_list[0].get_weights())
        #for i in range(1, num_clients):
         #   global_weights += np.array(model_list[i].get_weights())
        #new_global_weights = global_weights / num_clients

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)

        # Moved inside the num_epochs loop

        global_loss, global_accuracy = model_list[0].evaluate(X_test, y_test, verbose=0)
        global_losses.append(global_loss)
        global_accuracies.append(global_accuracy)
        print(f"Epoch {epoch + 1} - Global accuracy: {global_accuracy}")
        print(f"Epoch {epoch + 1} - Global loss: {global_loss}")

    return global_accuracies, global_losses
# Hyperparameters and the algorithm's execution
num_clients = 20
num_clients_to_select = 10
num_epochs = 20
client_epochs = 10
batch_size = 16
lr = 0.01
global_accuracies, global_losses = GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr)

#global_accuracy = GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr)

print("\nTop-performing client payoffs for each round:")
for round_num, round_payoffs in enumerate(all_rounds_payoffs):
    sorted_round_payoffs = sorted(round_payoffs, reverse=True)
    print(f"Round {round_num + 1}:", sorted_round_payoffs[:num_clients_to_select])
print(len(global_accuracies))
print("\t\t\tFL")
for i in range(num_epochs):
    print("global accuracy",str(global_accuracies[i]))
