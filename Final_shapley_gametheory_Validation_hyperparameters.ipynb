

import pandas as pd
import numpy as np
import xgboost as xgb
import shap
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.datasets import load_breast_cancer

# Step 1: Load Dataset
data = load_breast_cancer(as_frame=True)
df = pd.concat([data.data, data.target], axis=1)
df.columns = np.append(data.feature_names, 'target')

# Step 2: Split Features and Target
X = df.drop('target', axis=1)
y = df['target']

# Step 3: Normalize Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Apply SMOTE for Data Augmentation (Appending 5 Times)
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_aug_list = []
y_aug_list = []

for _ in range(5):  # Apply SMOTE 5 times
    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
    X_aug_list.append(X_resampled)
    y_aug_list.append(y_resampled)

# Step 5: Concatenate All Augmented Data
X_final = np.vstack(X_aug_list)
y_final = np.hstack(y_aug_list)

# Convert to DataFrame
X_aug_df = pd.DataFrame(X_final, columns=data.feature_names)
df_augmented = pd.concat([X_aug_df, pd.Series(y_final, name='target')], axis=1)

# Save Augmented Dataset
augmented_path = "/content/augmented_dataset_new.csv"
df_augmented.to_csv(augmented_path, index=False)

# Step 6: Train XGBoost Model on Augmented Data
model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
model.fit(X_final, y_final)

# Step 7: Compute SHAP Values for Feature Selection
explainer = shap.Explainer(model)
shap_values = explainer(X_final)

# Step 8: Extract Top 10 Features Based on SHAP Values
mean_shap_values = np.mean(np.abs(shap_values.values), axis=0)
feature_importance = pd.DataFrame(list(zip(data.feature_names, mean_shap_values)), columns=['feature', 'importance'])
feature_importance = feature_importance.sort_values('importance', ascending=False)
top_features = feature_importance['feature'].head(10).tolist()

# Step 9: Select Top Features
X_selected = X_aug_df[top_features]

# Step 10: Save Dataset with Selected Features
selected_features_path = "/content/selected_features_dataset.csv"
df_selected = pd.concat([X_selected, pd.Series(y_final, name='target')], axis=1)
df_selected.to_csv(selected_features_path, index=False)

print(f"Augmented dataset saved: {augmented_path}")
print(f"Dataset after SHAP feature selection saved: {selected_features_path}")


import pandas as pd
import numpy as np
import xgboost as xgb
!pip install shap
import shap
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
plt.rcParams.update({'font.family':'Nimbus Roman'})
from sklearn.metrics import precision_recall_fscore_support
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
# Load the breast cancer dataset into a pandas dataframe
data = load_breast_cancer(as_frame=True)
df = pd.concat([data.data, data.target], axis=1)
df.columns = np.append(data.feature_names, 'target')

# Split the dataset into features and target variable
X = df.drop('target', axis=1)
y = df['target']

# Train an XGBoost model on the dataset
model = xgb.XGBClassifier()
model.fit(X, y)

# Use SHAP values to determine feature importance
explainer = shap.Explainer(model)
shap_values = explainer(X)
shap.summary_plot(shap_values, X,show=False)
#shap.plots.bar(shap_values)
plt.savefig("accuracy.pdf", dpi=300)
# Extract the shapely values for each feature
mean_shap_values = np.mean(np.abs(shap_values.values), axis=0)

# Sort the features by shapely value in descending order
feature_importance = pd.DataFrame(list(zip(X.columns, mean_shap_values)), columns=['feature', 'importance'])
feature_importance = feature_importance.sort_values('importance', ascending=False)

# Print the top 10 features by shapely value
print(feature_importance.head(10))
# Select the top 10 features
top_features = feature_importance['feature'].head(10).tolist()
# Create a new dataframe with only the top features
X_top = X[top_features]

# Split the dataset into training and testing sets
X_temp, X_test, y_temp, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2
#X_temp, X_test, y_temp, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)
#X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)
#X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)

import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
from keras.constraints import maxnorm
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
from sklearn.metrics import roc_auc_score, roc_curve
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint

import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
from keras.constraints import maxnorm
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
from sklearn.metrics import roc_auc_score, roc_curve
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint

import numpy as np
import tensorflow as tf
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score

# Initialize structures for metrics storage
serverhist1 = {
    "loss": [],
    "accuracy": [],
    "payoff": [],
    "f1_score": [],
    "precision": [],
    "recall": [],
    "auc": []
}
client_payoff = []
client_number = []
all_rounds_payoffs = []
confusion_matrices = []
global_accuracies = []
global_accuracy_final = []
global_losses = []
auc_list = []
client_performances = []
def GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr):
    input_shape = X_train.shape[1:]
    model_list = []

    for i in range(num_clients):
        model = tf.keras.models.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=input_shape),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])
        model_list.append(model)

    # Added lists to store global metrics
    global_losses = []
    global_accuracies = []

    for epoch in range(num_epochs):
        for i in range(num_clients):
            history=model_list[i].fit(
                X_train[i * (len(X_train) // num_clients):(i + 1) * (len(X_train) // num_clients)],
                y_train[i * (len(X_train) // num_clients):(i + 1) * (len(X_train) // num_clients)],
                epochs=client_epochs,
                batch_size=batch_size,
                validation_data=(X_val, y_val)
            )
            training_loss = history.history['loss'][-1]
            training_accuracy = history.history['accuracy'][-1]
            val_loss = history.history['val_loss'][-1]
            val_accuracy = history.history['val_accuracy'][-1]

            print(f"Client {i+1} Epoch {epoch+1} - Training Loss: {training_loss:.4f}")
            print(f"Client {i+1} Epoch {epoch+1} - Training Accuracy: {training_accuracy:.4f}")
            print(f"Client {i+1} Epoch {epoch+1} - Validation Loss: {val_loss:.4f}")
            print(f"Client {i+1} Epoch {epoch+1} - Validation Accuracy: {val_accuracy:.4f}")
            y_pred = model_list[i].predict(X_test)
            y_pred_binary = (y_pred > 0.5).astype(int)
            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_binary, average='binary')
            cm = confusion_matrix(y_test, y_pred_binary)
            auc = roc_auc_score(y_test, y_pred)

            h = model_list[i].evaluate(X_test, y_test)

            serverhist1['loss'].append(h[0])
            serverhist1['accuracy'].append(h[1])
            serverhist1['precision'].append(precision)
            serverhist1['recall'].append(recall)
            serverhist1['f1_score'].append(f1)
            serverhist1['auc'].append(auc)

        client_accuracies = []
        for i in range(num_clients):
            _, accuracy = model_list[i].evaluate(X_test, y_test, verbose=0)
            client_accuracies.append(accuracy)

        client_payoffs = []
        for i in range(num_clients):
            payoff = client_accuracies[i] - np.mean(client_accuracies)
            client_payoffs.append(payoff)
            client_payoff.append(payoff)
            client_number.append(i)
            serverhist1['payoff'].append(client_payoffs[i])

        all_rounds_payoffs.append(client_payoffs)
        sorted_clients = sorted(range(num_clients), key=lambda x: client_payoffs[x], reverse=True)
        print("sorted clients",sorted_clients)
        selected_clients = sorted_clients[:num_clients_to_select]
        selected_payoffs = [client_payoffs[i] for i in selected_clients]
        weights = [client_payoffs[i] for i in selected_clients]
        print("Payoffs of selected clients:", selected_payoffs)
        weights /= np.sum(weights)

        #sorted_clients = sorted(range(num_clients), key=lambda x: client_payoffs[x], reverse=True)
        #print("sorted clients",sorted_clients)
        #selected_clients = sorted_clients[:num_clients_to_select]
        #print("selected clients len", selected_clients)
        #selected_payoffs = [client_payoffs[i] for i in selected_clients]
        #weights = [client_payoffs[i] for i in selected_clients]
        #print("Payoffs of selected clients:", selected_payoffs)
        #weights /= np.sum(weights)

        #global_weights = np.zeros_like(model_list[0].get_weights())
        #for i in range(num_clients_to_select):
         #   global_weights += weights[i] * np.array(model_list[i].get_weights())
        #new_global_weights = global_weights / num_clients_to_select
        # Aggregate client model weights based on their performance/payoff
        global_weights = np.zeros_like(model_list[0].get_weights())
        for i in range(num_clients_to_select):
            global_weights += weights[i] * np.array(model_list[i].get_weights())
        new_global_weights = global_weights / num_clients_to_select

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)
          #  for layer in model_list[i].layers:
           #     layer.set_weights(layer.get_weights() + lr * client_payoffs[i])

        #global_weights = np.array(model_list[0].get_weights())
        for i in range(1, num_clients):
            global_weights += np.array(model_list[i].get_weights())
        new_global_weights = global_weights / num_clients

        for i in range(num_clients):
            model_list[i].set_weights(new_global_weights)

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)
          #  for layer in model_list[i].layers:
           #     layer.set_weights(layer.get_weights() + lr * client_payoffs[i])

        #global_weights = np.array(model_list[0].get_weights())
        #for i in range(1, num_clients):
         #   global_weights += np.array(model_list[i].get_weights())
        #new_global_weights = global_weights / num_clients

        #for i in range(num_clients):
         #   model_list[i].set_weights(new_global_weights)

        # Moved inside the num_epochs loop

        global_loss, global_accuracy = model_list[0].evaluate(X_test, y_test, verbose=0)
        global_losses.append(global_loss)
        global_accuracies.append(global_accuracy)
        print(f"Epoch {epoch + 1} - Global accuracy: {global_accuracy}")
        print(f"Epoch {epoch + 1} - Global loss: {global_loss}")

    return global_accuracies, global_losses
# Hyperparameters and the algorithm's execution
num_clients = 20
num_clients_to_select = 10
num_epochs = 10
client_epochs = 10
learning_rates = [0.01, 0.001,0.1]
batch_sizes = [16, 32]

results = {}

for lr in learning_rates:
    for bs in batch_sizes:
        print(f"Training with lr={lr} and batch_size={bs}")
        global_accuracies, global_losses = GTFL(X_train, y_train, X_test, y_test, X_val, y_val, num_clients, num_epochs, bs, lr)
        key = f"lr={lr}_bs={bs}"
        results[key] = {
            "global_accuracies": global_accuracies,
            "client_performances": client_performances
        }

    #avg_client_accuracy = np.mean(value["client_performances"], axis=1)
    #for epoch, acc in enumerate(avg_client_accuracy):
     #   print(f"Epoch {epoch+1} - Average Client Accuracy: {acc:.4f}")
#global_accuracies, global_losses = GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr)

#global_accuracy = GTFL(X_train, y_train, X_test, y_test,X_val,y_val, num_clients, num_epochs, batch_size, lr)

print("\nTop-performing client payoffs for each round:")
for round_num, round_payoffs in enumerate(all_rounds_payoffs):
    sorted_round_payoffs = sorted(round_payoffs, reverse=True)
    print(f"Round {round_num + 1}:", sorted_round_payoffs[:num_clients_to_select])
print(len(global_accuracies))
print("\t\t\tFL")
for i in range(num_epochs):
    print("global accuracy",str(global_accuracies[i]))
    print("global loss",str(global_losses[i]))


import matplotlib.pyplot as plt

# Accuracy values for the first 10 epochs
accuracy_values = [
    0.37719297409057617,
    0.6228070259094238,
    0.9210526347160339,
    0.8421052694320679,
    0.9385964870452881,
    0.9385964870452881,
    0.9385964870452881,
    0.9210526347160339,
    0.9210526347160339,
    0.9298245906829834
]

# Epoch numbers
epochs = list(range(1, 11))

# Plotting
plt.figure(figsize=(10,6))
plt.plot(epochs, accuracy_values, marker='o', linestyle='-', color='b')
plt.title("Model Accuracy over 10 Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.xticks(epochs)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt

# Accuracy values for all 20 epochs
accuracy_values = [
    0.37719297409057617,
    0.6228070259094238,
    0.9210526347160339,
    0.8421052694320679,
    0.9385964870452881,
    0.9385964870452881,
    0.9385964870452881,
    0.9210526347160339,
    0.9210526347160339,
    0.9298245906829834,
    0.9298245906829834,
    0.9473684430122375,
    # ... you can continue with the rest of the values, but they're not necessary for this plot
]

# Filter out the epochs until the accuracy reaches 0.947 for the first time
filtered_values = []
for value in accuracy_values:
    filtered_values.append(value)
    if value >= 0.947:
        break

# Epoch numbers for filtered values
epochs = list(range(1, len(filtered_values) + 1))

# Plotting
plt.figure(figsize=(10,6))
plt.plot(epochs, filtered_values, marker='o', linestyle='-', color='b')
#plt.title("Model Accuracy until 94.7% is Achieved")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.xticks(epochs)
plt.axhline(y=0.947, color='r', linestyle='--', label="94.7% threshold")
#plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt

# Provided global loss values
loss_values = [
    0.851537823677063, 0.695427417755127, 0.6187029480934143, 0.5749087929725647,
    0.5374301671981812, 0.4980679452419281, 0.4555768370628357, 0.42724302411079407,
    0.39677104353904724, 0.3636799454689026, 0.32991600036621094, 0.31033799052238464
]

# Corresponding epochs
epochs = list(range(1, len(loss_values) + 1))

# Plotting the values
plt.figure(figsize=(10, 6))
plt.plot(epochs, loss_values, marker='o', color='blue')
#plt.title('Global Loss Values Across Epochs')
plt.xlabel('Epoch')
plt.ylabel('Global Loss')
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()

# Save the plot to a file (optional)
# plt.savefig('loss_plot.png', dpi=300)

# Display the plot
plt.show()


import matplotlib.pyplot as plt

# Provided payoffs of selected clients
payoffs = [
    0.1135964900255203, 0.1135964900255203, 0.10482459366321562, 0.08728068172931669,
    0.078508785367012, 0.078508785367012, 0.078508785367012, 0.06096493303775785,
    0.025877168774604775, 0.017105272412300088
]

# Corresponding clients
clients = [f"Client {i+1}" for i in range(len(payoffs))]

# Plotting the payoffs
plt.figure(figsize=(12, 7))
plt.bar(clients, payoffs, color='skyblue')
plt.title('Payoffs of Selected Clients')
plt.xlabel('Clients')
plt.ylabel('Payoff')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot to a file (optional)
plt.savefig('client_payoffs.pdf', dpi=300)

# Display the plot
plt.show()


import matplotlib.pyplot as plt

# Accuracy values
accuracy_values = [
    0.37719297409057617,
    0.6228070259094238,
    0.9210526347160339,
    0.8421052694320679,
    0.9385964870452881,
    0.9385964870452881,
    0.9385964870452881,
    0.9210526347160339,
    0.9210526347160339,
    0.9298245906829834,
    0.9298245906829834,
    0.9473684430122375
]

# Global loss values
loss_values = [
    0.851537823677063, 0.695427417755127, 0.6187029480934143, 0.5749087929725647,
    0.5374301671981812, 0.4980679452419281, 0.4555768370628357, 0.42724302411079407,
    0.39677104353904724, 0.3636799454689026, 0.32991600036621094, 0.31033799052238464
]

# Epochs for both values
epochs = list(range(1, len(loss_values) + 1))

# Create subplots
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))

# Plot accuracy
ax1.plot(epochs, accuracy_values, marker='o', linestyle='-', color='b')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Accuracy')
ax1.axhline(y=0.947, color='r', linestyle='--', label="94.7% threshold")
ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

# Plot loss values
ax2.plot(epochs, loss_values, marker='o', color='blue')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Global Loss')
ax2.grid(True, which='both', linestyle='--', linewidth=0.5)

# Adjust layout
plt.tight_layout()

# Save to a PDF
plt.savefig('combined_plot.pdf', format='pdf')

# Display the plot
plt.show()


import matplotlib.pyplot as plt

# Accuracy values
accuracy_values = [
    0.37719297409057617,
    0.6228070259094238,
    0.9210526347160339,
    0.8421052694320679,
    0.9385964870452881,
    0.9385964870452881,
    0.9385964870452881,
    0.9210526347160339,
    0.9210526347160339,
    0.9298245906829834,
    0.9298245906829834,
    0.9473684430122375
]

# Global loss values
loss_values = [
    0.851537823677063, 0.695427417755127, 0.6187029480934143, 0.5749087929725647,
    0.5374301671981812, 0.4980679452419281, 0.4555768370628357, 0.42724302411079407,
    0.39677104353904724, 0.3636799454689026, 0.32991600036621094, 0.31033799052238464
]

# Epochs for both values
epochs = list(range(1, len(loss_values) + 1))

# Create a plot
fig, ax1 = plt.subplots(figsize=(12, 6))

# Plot accuracy on the left y-axis
ax1.plot(epochs, accuracy_values, marker='o', linestyle='-', color='b', label='Accuracy')
ax1.axhline(y=0.947, color='r', linestyle='--', label="94.7% threshold")
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Accuracy', color='b')
ax1.tick_params(axis='y', labelcolor='b')
ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

# Create a second y-axis to plot loss on the right
ax2 = ax1.twinx()
ax2.plot(epochs, loss_values, marker='o', color='green', label='Global Loss')
ax2.set_ylabel('Global Loss', color='green')
ax2.tick_params(axis='y', labelcolor='green')

# Show plot
plt.title('Accuracy and Global Loss Across Epochs')
plt.tight_layout()
plt.savefig('combined_plot.pdf', format='pdf')
plt.show()


import matplotlib.pyplot as plt

# Accuracy values
accuracy_values = [
    0.37719297409057617,
    0.6228070259094238,
    0.9210526347160339,
    0.8421052694320679,
    0.9385964870452881,
    0.9385964870452881,
    0.9385964870452881,
    0.9210526347160339,
    0.9210526347160339,
    0.9298245906829834,
    0.9298245906829834,
    0.9473684430122375
]

# Global loss values
loss_values = [
    0.851537823677063, 0.695427417755127, 0.6187029480934143, 0.5749087929725647,
    0.5374301671981812, 0.4980679452419281, 0.4555768370628357, 0.42724302411079407,
    0.39677104353904724, 0.3636799454689026, 0.32991600036621094, 0.31033799052238464
]

# Epochs for both values
epochs = list(range(1, len(loss_values) + 1))

# Create a grid layout for two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot accuracy on ax1
ax1.plot(epochs, accuracy_values, marker='o', linestyle='-', color='b')
ax1.axhline(y=0.947, color='r', linestyle='--', label="94.7% threshold")
ax1.set_title('Accuracy')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Accuracy')
ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

# Plot loss values on ax2
ax2.plot(epochs, loss_values, marker='o', color='green')
ax2.set_title('Loss')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.grid(True, which='both', linestyle='--', linewidth=0.5)

# Adjust layout and save to a PDF
plt.tight_layout()
plt.savefig('side_by_side_plot.pdf', format='pdf')

# Display the plots
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Data from the experiments (same as in previous response)
experiments = [
    {"lr": 0.01, "batch_size": 16, "accuracy": [0.62, 0.41, 0.61, 0.94, 0.94, 0.85, 0.95, 0.94, 0.90, 0.92]},
    {"lr": 0.01, "batch_size": 32, "accuracy": [0.38, 0.62, 0.40, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94]},
    {"lr": 0.001, "batch_size": 16, "accuracy": [0.38, 0.38, 0.39, 0.78, 0.84, 0.93, 0.95, 0.93, 0.95, 0.93]},
    {"lr": 0.001, "batch_size": 32, "accuracy": [0.62, 0.38, 0.87, 0.79, 0.93, 0.94, 0.94, 0.93, 0.94, 0.93]},
    {"lr": 0.1, "batch_size": 16, "accuracy": [0.38, 0.82, 0.39, 0.46, 0.53, 0.94, 0.81, 0.89, 0.94, 0.90]},
    {"lr": 0.1, "batch_size": 32, "accuracy": [0.38, 0.61, 0.39, 0.90, 0.94, 0.94, 0.93, 0.93, 0.93, 0.93]}
]

# Convert the data to a DataFrame
data = []
for exp in experiments:
    for acc in exp["accuracy"]:
        data.append({"Learning Rate": exp["lr"], "Batch Size": exp["batch_size"], "Accuracy": acc})
df = pd.DataFrame(data)

# Set up the figure and style
plt.figure(figsize=(10, 6))
sns.set(style="whitegrid")

# Create the bar plot using Seaborn
bar = sns.barplot(data=df, x="Learning Rate", y="Accuracy", hue="Batch Size", ci=None, palette="viridis")

# Customize the plot
bar.set(title="Accuracy for Different Hyperparameters")
bar.set_xlabel("Learning Rate")
bar.set_ylabel("Accuracy")

# Customize legend
legend = bar.legend(title="Batch Size")
legend.set_labels(["16", "32"])

# Tight layout for better presentation
plt.tight_layout()

# Save the plot to a file
plt.savefig("research_paper_bar_graph.png")

# Show the plot
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Data from the experiments (same as in previous response)
experiments = [
    {"lr": 0.01, "batch_size": 16, "accuracy": [0.62, 0.41, 0.61, 0.94, 0.94, 0.85, 0.95, 0.94, 0.90, 0.92], "loss": [0.87, 0.62, 0.58, 0.54, 0.50, 0.47, 0.43, 0.40, 0.39, 0.36]},
    # ... other experiments ...
]

# Convert the data to a DataFrame
data = []
for exp in experiments:
    best_accuracy = max(exp["accuracy"])
    best_accuracy_index = exp["accuracy"].index(best_accuracy)
    best_loss = min(exp["loss"])
    best_loss_index = exp["loss"].index(best_loss)
    data.append({"Learning Rate": exp["lr"], "Batch Size": exp["batch_size"],
                 "Best Accuracy": best_accuracy, "Best Loss": best_loss})

df = pd.DataFrame(data)

# Set up the figure and style
sns.set(style="whitegrid")

# Create the subplot for accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
accuracy_plot = sns.barplot(data=df, x="Learning Rate", y="Best Accuracy", hue="Batch Size", ci=None, palette="viridis")
accuracy_plot.set(title="Best Global Accuracy")
accuracy_plot.set_xlabel("Learning Rate")
accuracy_plot.set_ylabel("Accuracy")
accuracy_plot.legend(title="Batch Size")

# Create the subplot for loss
plt.subplot(1, 2, 2)
loss_plot = sns.barplot(data=df, x="Learning Rate", y="Best Loss", hue="Batch Size", ci=None, palette="viridis")
loss_plot.set(title="Best Global Loss")
loss_plot.set_xlabel("Learning Rate")
loss_plot.set_ylabel("Loss")
loss_plot.legend(title="Batch Size")

# Adjust layout for better presentation
plt.tight_layout()

# Save the plot to a file
plt.savefig("best_performing_scenarios.png")

# Show the plot
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Data from the experiments (same as in previous response)
experiments = [
    {"lr": 0.01, "batch_size": 16, "accuracy": [0.62, 0.41, 0.61, 0.94, 0.94, 0.85, 0.95, 0.94, 0.90, 0.92], "loss": [0.87, 0.62, 0.58, 0.54, 0.50, 0.47, 0.43, 0.40, 0.39, 0.36]},
    # ... other experiments ...
]

# Find the experiment for lr=0.01 and batch_size=16
target_experiment = None
for exp in experiments:
    if exp["lr"] == 0.01 and exp["batch_size"] == 16:
        target_experiment = exp
        break

if target_experiment is None:
    print("Experiment not found for lr=0.01 and batch_size=16")
    exit()

# Convert accuracy and loss data to a DataFrame
data = {
    "Epoch": list(range(1, 11)),
    "Global Accuracy": target_experiment["accuracy"],
    "Global Loss": target_experiment["loss"]
}
df = pd.DataFrame(data)

# Set up the figure and style
sns.set(style="whitegrid")

# Create the subplot for global accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
accuracy_plot = sns.lineplot(data=df, x="Epoch", y="Global Accuracy")
accuracy_plot.set(title="Global Accuracy (LR=0.01, Batch Size=16)")
accuracy_plot.set_xlabel("Epoch")
accuracy_plot.set_ylabel("Accuracy")

# Create the subplot for global loss
plt.subplot(1, 2, 2)
loss_plot = sns.lineplot(data=df, x="Epoch", y="Global Loss", color="orange")
loss_plot.set(title="Global Loss (LR=0.01, Batch Size=16)")
loss_plot.set_xlabel("Epoch")
loss_plot.set_ylabel("Loss")

# Adjust layout for better presentation
plt.tight_layout()

# Save the plot to a file
plt.savefig("lr_0.01_batch_16_line_graphs.png")

# Show the plot
plt.show()


import matplotlib.pyplot as plt

# Provided data
epochs = list(range(1, 11))
accuracy = [0.6228070259094238, 0.41228070855140686, 0.6140350699424744, 0.9385964870452881, 0.9385964870452881,
            0.859649121761322, 0.9473684430122375, 0.9385964870452881, 0.9035087823867798, 0.9210526347160339]

loss = [0.8723797798156738, 0.6247066259384155, 0.5796641707420349, 0.5359064936637878, 0.4982299208641052,
        0.4768475890159607, 0.43377137184143066, 0.40426602959632874, 0.39281877875328064, 0.36288389563560486]

# Create the line graph
plt.figure(figsize=(10, 6))

# Line plot for global accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, accuracy, marker='o')
plt.title("Global Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")

# Line plot for global loss
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, marker='o', color='orange')
plt.title("Global Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")

# Adjust layout for better presentation
plt.tight_layout()

# Show the plot
plt.show()


import matplotlib.pyplot as plt

# Extracted data from your text
epochs = list(range(1, 11))

# Data for lr=0.01 and batch_size=16
accuracy_001_16 = [0.6228070259094238, 0.41228070855140686, 0.6140350699424744, 0.9385964870452881, 0.9385964870452881, 0.859649121761322, 0.9473684430122375, 0.9385964870452881, 0.9035087823867798, 0.9210526347160339]
loss_001_16 = [0.8723797798156738, 0.6247066259384155, 0.5796641707420349, 0.5359064936637878, 0.4982299208641052, 0.4768475890159607, 0.43377137184143066, 0.40426602959632874, 0.39281877875328064, 0.36288389563560486]

# [Similarly, you can extract data for other combinations]

# Plotting
plt.figure(figsize=(12, 6))

# Accuracy subplot
plt.subplot(1, 2, 1)
plt.plot(epochs, accuracy_001_16, marker='o', label='lr=0.01, batch_size=16')
# [Similarly, plot for other combinations]
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss subplot
plt.subplot(1, 2, 2)
plt.plot(epochs, loss_001_16, marker='o', label='lr=0.01, batch_size=16')
# [Similarly, plot for other combinations]
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Data
epochs = [i for i in range(1, 11)]
global_accuracy = [0.37719, 0.62281, 0.92105, 0.84211, 0.93860, 0.93860, 0.92105, 0.92105, 0.92982, 0.94737]
global_loss = [0.85154, 0.69543, 0.61870, 0.57491, 0.53743, 0.49807, 0.45558, 0.42724, 0.39677, 0.31034]

fig, ax = plt.subplots(1, 2, figsize=(15, 6))  # 1 row, 2 columns

# Global Accuracy Plot
ax[0].plot(epochs, global_accuracy, marker='o', color='blue')
ax[0].set_xlabel('Epoch')
ax[0].set_ylabel('Accuracy')
#ax[0].set_title('Global Accuracy over Epochs')
ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)

# Global Loss Plot
ax[1].plot(epochs, global_loss, marker='s', color='red')
ax[1].set_xlabel('Epoch')
ax[1].set_ylabel('Loss')
#ax[1].set_title('Global Loss over Epochs')
ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)

plt.tight_layout()
plt.savefig("Final-results.pdf")  # Save the figure
plt.show()
